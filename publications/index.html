<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | SeongHak KIM</title> <meta name="author" content="SeongHak KIM"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%98%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://seonghak35.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">SeongHak </span>KIM</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">SeongHak</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/leaderboard/">Leaderboard</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:blue"><a href="https://www.sciencedirect.com/journal/engineering-applications-of-artificial-intelligence" rel="external nofollow noopener" target="_blank">EAAI</a></abbr></div> <div id="EAAI" class="col-sm-8"> <div class="title">Teach Sample-Specific Knowledge: Seperated Distillation on the Correct and Incorrect samples</div> <div class="author"> <em>Seonghak Kim</em>, Gyeongdo Ham, Suin Lee, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Daeshik Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Engineering Applications of Artificial Intelligence</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Knowledge distillation (KD) methods enhance the performance of lightweight student models by transferring knowledge from powerful but complex teacher models for real-time application. Traditional logit-based KD methods use forward Kullback-Leibler divergence (FKLD) to transfer meaningful knowledge. However, FKLD typically exhibits a mode-averaging property, causing students to focus on non-target information, whether the teacher’s samples are correct or incorrect. Additionally, when handling uncertain samples, even teacher models may fail to classify them accurately, leading to incorrect predictions and confusing the students. To address these issues, we classify the dataset into two groups based on the teacher’s predictions: correct and incorrect samples. To ensure a more reliable transfer of knowledge from teacher to student for correct samples, we employ both forward Kullback-Leibler divergence (FKLD) and reverse Kullback-Leibler divergence (RKLD), which has mode-focusing properties. We also reduce temperature scaling to maximize information about the target. Conversely, for incorrect predictions, our method minimizes the teacher’s knowledge, encouraging students to rely more on the true labels by focusing on cross-entropy loss. Experimental results on both classification and object detection tasks demonstrate that the our method, Teach Sample-Specific Knowledge (TSSK), outperforms state-of-the-art KD methods, making it ideal for deployment on-devices in real-world scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="kim2025biased" class="col-sm-8"> <div class="title">Biased Teacher, Balanced Student</div> <div class="author"> <em>Seonghak Kim</em> </div> <div class="periodical"> <em>arXiv preprint arXiv:2506.18496</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.18496" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Knowledge Distillation (KD) is a widely adopted model compression technique where a compact student model learns from the output of a larger, pre-trained teacher. While effective in balanced settings, conventional KD suffers significantly when applied to long-tailed data distributions, as the teacher model tends to be biased toward head classes and provides limited supervision for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation (LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by reformulating the standard KD objective into two components: inter-group and intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction distributions across and within class groups (head, medium, tail), respectively. This decomposition allows us to identify and quantify the sources of teacher bias. To address them, we introduce (1) a rebalanced inter-group loss that calibrates the teacher’s group-level predictions and (2) a uniform intra-group loss that ensures equal contribution from all groups during distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and ImageNet-LT show that LTKD consistently outperforms existing KD methods, achieving significant gains in both overall accuracy and tail-class performance. Our results demonstrate that LTKD enables effective knowledge transfer even from biased teachers, making it a strong candidate for real-world deployment in resource-constrained and imbalanced settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="kim2025figkd" class="col-sm-8"> <div class="title">FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer</div> <div class="author"> <em>Seonghak Kim</em> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.11897</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.11897" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Knowledge distillation (KD) is a widely adopted technique for transferring knowledge from a high-capacity teacher model to a smaller student model by aligning their output distributions. However, existing methods often underperform in fine-grained visual recognition tasks, where distinguishing subtle differences between visually similar classes is essential. This performance gap stems from the fact that conventional approaches treat the teacher’s output logits as a single, undifferentiated signal-assuming all contained information is equally beneficial to the student. Consequently, student models may become overloaded with redundant signals and fail to capture the teacher’s nuanced decision boundaries. To address this issue, we propose Fine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework that decomposes a model’s logits into low-frequency (content) and high-frequency (detail) components using the discrete wavelet transform (DWT). FiGKD selectively transfers only the high-frequency components, which encode the teacher’s semantic decision patterns, while discarding redundant low-frequency content already conveyed through ground-truth supervision. Our approach is simple, architecture-agnostic, and requires no access to intermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet, and multiple fine-grained recognition benchmarks show that FiGKD consistently outperforms state-of-the-art logit-based and feature-based distillation methods across a variety of teacher-student configurations. These findings confirm that frequency-aware logit decomposition enables more efficient and effective knowledge transfer, particularly in resource-constrained settings.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:blue"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69" rel="external nofollow noopener" target="_blank">TKDE</a></abbr></div> <div id="KIM10623281" class="col-sm-8"> <div class="title">Robustness-Reinforced Knowledge Distillation with Correlation Distance and Network Pruning</div> <div class="author"> <em>Seonghak Kim</em>, Gyeongdo Ham, Yucheol Cho, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Daeshik Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Knowledge and Data Engineering</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TKDE.2024.3438074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The improvement in the performance of efficient and lightweight models (i.e., the student model) is achieved through knowledge distillation (KD), which involves transferring knowledge from more complex models (i.e., the teacher model). However, most existing KD techniques rely on Kullback-Leibler (KL) divergence, which has certain limitations. First, if the teacher distribution has high entropy, the KL divergence’s mode-averaging nature hinders the transfer of sufficient target information. Second, when the teacher distribution has low entropy, the KL divergence tends to excessively focus on specific modes, which fails to convey an abundant amount of valuable knowledge to the student. Consequently, when dealing with datasets that contain numerous confounding or challenging samples, student models may struggle to acquire sufficient knowledge, resulting in subpar performance. Furthermore, in previous KD approaches, we observed that data augmentation, a technique aimed at enhancing a model’s generalization, can have an adverse impact. Therefore, we propose a Robustness-Reinforced Knowledge Distillation (R2KD) that leverages correlation distance and network pruning. This approach enables KD to effectively incorporate data augmentation for performance improvement. Extensive experiments on various datasets, including CIFAR-100, FGVR, TinyImagenet, and ImageNet, demonstrate our method’s superiority over current state-of-the-art methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:blue"><a href="https://www.sciencedirect.com/journal/knowledge-based-systems" rel="external nofollow noopener" target="_blank">KBS</a></abbr></div> <div id="KIM2024111911" class="col-sm-8"> <div class="title">Maximizing discrimination capability of knowledge distillation with energy function</div> <div class="author"> <em>Seonghak Kim</em>, Gyeongdo Ham, Suin Lee, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Donggon Jang, Daeshik Kim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Knowledge-Based Systems</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.knosys.2024.111911" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>To apply the latest computer vision techniques that require a large computational cost in real industrial applications, knowledge distillation methods (KDs) are essential. Existing logit-based KDs apply the constant temperature scaling to all samples in dataset, limiting the utilization of knowledge inherent in each sample individually. In our approach, we classify the dataset into two categories (i.e., low energy and high energy samples) based on their energy score. Through experiments, we have confirmed that low energy samples exhibit high confidence scores, indicating certain predictions, while high energy samples yield low confidence scores, meaning uncertain predictions. To distill optimal knowledge by adjusting non-target class predictions, we apply a higher temperature to low energy samples to create smoother distributions and a lower temperature to high energy samples to achieve sharper distributions. When compared to previous logit-based and feature-based methods, our energy-based KD (Energy KD) achieves better performance on various datasets. Especially, Energy KD shows significant improvements on CIFAR-100-LT and ImageNet datasets, which contain many challenging samples. Furthermore, we propose high energy-based data augmentation (HE-DA) for further improving the performance. We demonstrate that higher performance improvement could be achieved by augmenting only a portion of the dataset rather than the entire dataset, suggesting that it can be employed on resource-limited devices. To the best of our knowledge, this paper represents the first attempt to make use of energy function in knowledge distillation and data augmentation, and we believe it will greatly contribute to future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:gray"><a href="">KSR</a></abbr></div> <div id="ksr" class="col-sm-8"> <div class="title">Computational analysis on electrokinetic flow fields of electrolytic solutions in polyelectrolyte brush-grafted channels</div> <div class="author"> <em>Seonghak Kim</em>, and Myung-Suk Chun</div> <div class="periodical"> <em>In Korean Society of Rheology 2024 Spring Conference</em>, May 2024 </div> <div class="periodical"> Seoul, Seoul, ROK </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#6495ED"><a href="https://www.aiaa.org/events-learning/event/2024/01/08/default-calendar/2024-aiaa-science-and-technology-forum-and-exposition-(aiaa-scitech-forum)" rel="external nofollow noopener" target="_blank">AIAA</a></abbr></div> <div id="aiaa" class="col-sm-8"> <div class="title">Influence of the non-isothermal phase change on cavitation bubble dynamics</div> <div class="author"> Kyungjun Choi, <em>Seonghak Kim</em>, Hyunji Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chongam Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In American Institute of Aeronautics and Astronautics SciTech</em>, Jan 2024 </div> <div class="periodical"> Orlando, FL, USA </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#2F4F4F"><a href="https://wacv2024.thecvf.com/" rel="external nofollow noopener" target="_blank">WACV</a></abbr></div> <div id="wacv" class="col-sm-8"> <div class="title">Robust Unsupervised Domain Adaptation through Negative-View Regularization</div> <div class="author"> Joonhyeok Jang*, Shunhyeok Lee*, <em>Seonghak Kim</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jungun Kim, Seonghyun Kim, Daeshik Kim' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Winter Conference on Applications of Computer Vision</em>, Jan 2024 </div> <div class="periodical"> Waikoloa, HI, USA </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ArXiv</abbr></div> <div id="ham2023cosine" class="col-sm-8"> <div class="title">Cosine Similarity Knowledge Distillation for Individual Class Information Transfer</div> <div class="author"> <em>Seonghak Kim</em>, Gyeongdo Ham, Suin Lee, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jae-Hyeok Lee, Daeshik Kim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2311.14307</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.14307" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Previous logits-based Knowledge Distillation (KD) have utilized predictions about multiple categories within each sample (i.e., class predictions) and have employed Kullback-Leibler (KL) divergence to reduce the discrepancy between the student and teacher predictions. Despite the proliferation of KD techniques, the student model continues to fall short of achieving a similar level as teachers. In response, we introduce a novel and effective KD method capable of achieving results on par with or superior to the teacher models performance. We utilize teacher and student predictions about multiple samples for each category (i.e., batch predictions) and apply cosine similarity, a commonly used technique in Natural Language Processing (NLP) for measuring the resemblance between text embeddings. This metric’s inherent scale-invariance property, which relies solely on vector direction and not magnitude, allows the student to dynamically learn from the teacher’s knowledge, rather than being bound by a fixed distribution of the teacher’s knowledge. Furthermore, we propose a method called cosine similarity weighted temperature (CSWT) to improve the performance. CSWT reduces the temperature scaling in KD when the cosine similarity between the student and teacher models is high, and conversely, it increases the temperature scaling when the cosine similarity is low. This adjustment optimizes the transfer of information from the teacher to the student model. Extensive experimental results show that our proposed method serves as a viable alternative to existing methods. We anticipate that this approach will offer valuable insights for future research on model compression.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:green"><a href="https://www.sciencedirect.com/journal/ocean-engineering" rel="external nofollow noopener" target="_blank">Ocean Eng.</a></abbr></div> <div id="choi2023computational" class="col-sm-8"> <div class="title">Computational investigation on the non-isothermal phase change during cavitation bubble pulsations</div> <div class="author"> Kyungjun Choi, <em>Seonghak Kim</em>, Hyunji Kim, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chongam Kim' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Ocean Engineering</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.oceaneng.2023.115414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>During cavitation bubble pulsations, a phase change intensively occurs near the collapsing moment due to high pressure and temperature inside bubbles, accompanying distinctive flow features: the rate of evaporation and condensation significantly changes according to the phase change regime. To account for this non-isothermal effect, the high-fidelity computational framework incorporating the physics-based cavitation model and a new fluid property model based on artificial neural network is proposed. The key finding of this study is the interplay between the thermal and inertial effects during multiple pulsations. At the early stages of bubble contraction, the phase change is primarily driven by fluid inertia. However, as the bubble continues to compress, the thermal effect becomes dominant and controls the entire phase change region at each moment of collapse. It is observed that the isothermal model relying on the inertial bubble growth rate only, does not capture this transition of dominance and eventually fails to predict multiple pulsations. The physics-based cavitation model successfully captures the bubble pulsation beyond the second collapse. These findings highlight that explicit consideration of the non-isothermal effect is essential for problems with varying phase change regimes, and a phase change model reflecting this effect is vital for accurate computations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:blue"><a href="https://link.springer.com/journal/42405" rel="external nofollow noopener" target="_blank">IJASS</a></abbr></div> <div id="kim2023effect" class="col-sm-8"> <div class="title">Effect of Non-isothermal Phase Change on Multiple Bubble Pulsations</div> <div class="author"> <em>Seonghak Kim</em>, Kyungjun Choi, and Chongam Kim</div> <div class="periodical"> <em>International Journal of Aeronautical and Space Sciences</em>, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s42405-023-00581-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Main reasons for the damage to submerged structures include shock waves and high-velocity jetting hits at the collapse of the cavitation bubbles, which repeatedly occur in subsequent bubble periods. Although other methods, e.g., experiments and theoretical approaches, have been conducted to gain knowledge of bubble dynamics, these approaches have difficulty in capturing microscopic phase changes and are not suitable for largely deforming motion, respectively. Therefore, numerical simulations using Navier–Stokes equations or Euler equations have been used to describe the bubble’s dynamic behaviors and detailed physical phenomena inside the bubble. Nevertheless, previous numerical simulations had limitations in expressing realistic and accurate bubble dynamics. For example, their results focused only on the first bubble period, not on multiple periods; thus, they could not obtain the information about the continuous shock loading near the structures. More noticeably, the thermal effect in multiple pulsations has never been addressed; since the pressure and temperature inside the bubble are formed near the critical point, the thermal effect has to be considered for accurate computations. Herein, the isothermal and non-isothermal phase change models are applied to observe the phase change effect and thermal effect on the bubble dynamics, respectively. Contrary to the isothermal model, which captures bubble dynamics up to the second bubble period, the non-isothermal model accurately expresses bubble dynamics up to the third bubble period, which is closely related to the thermal effect at the collapse region.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF1493"><a href="">ACFD</a></abbr></div> <div id="acfd" class="col-sm-8"> <div class="title">Non-Isothermal Phase Change Effect on the Multiple Bubble Pulsations</div> <div class="author"> <em>Seonghak Kim</em>, Kyungjun Choi, and Chongam Kim</div> <div class="periodical"> <em>In Asian Computational Fluid Dynamics Conference</em>, Oct 2022 </div> <div class="periodical"> Jeju, Jeju, ROK </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:gray"><a href="">S&amp;V</a></abbr></div> <div id="sv" class="col-sm-8"> <div class="title">The Phase Change and Thermal Effect on the Bubble Dynamics: First, Second and Third Bubble Pulsations</div> <div class="author"> <em>Seonghak Kim</em>, Kyungjun Choi, and Chongam Kim</div> <div class="periodical"> <em>In Shock and Vibration Symposium</em>, Sep 2022 </div> <div class="periodical"> Denver, CO, USA </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B22222"><a href="">NCFE</a></abbr></div> <div id="ncfe" class="col-sm-8"> <div class="title">Numerical analysis of a phase change effect on the multiple pulsations of the spark-generated bubble</div> <div class="author"> <em>Seonghak Kim</em>, Kyungjun Choi, and Chongam Kim</div> <div class="periodical"> <em>In National Congress on Fluids Engineering</em>, Jun 2022 </div> <div class="periodical"> Changwon, Gyeongsangnam-do, ROK </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#ADD8E6"><a href="">KSASS</a></abbr></div> <div id="ksass" class="col-sm-8"> <div class="title">Computational framework for simulation of the whole process of underwater explosion</div> <div class="author"> Kyungjun Choi, <em>Seonghak Kim</em>, and Chongam Kim</div> <div class="periodical"> <em>In Korean Society for Aeronautical &amp; Space Sciences 2021 Fall Conference</em>, Nov 2021 </div> <div class="periodical"> Jeju, Jeju, ROK </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 SeongHak KIM. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QZ6W7M2HLM"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-QZ6W7M2HLM");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>